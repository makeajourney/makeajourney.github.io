---
title: "Apache Spark"
layout: post
---


# Apache Spark

범용적, 빠른 속도록 작업을 수행할 수 있도록 설계한 클러스터용 연산 플랫폼.


---

## Component

<br/>

### Spark Core


---

## installation  

1. <http://spark.apache.org/downloads.html> 접속.  
2. 연동할 Hadoop version과 맞는 패키지 선택한 후 다운로드.  
  - HDFS(Hadoop Distributed File System)과 연동하려면 HDFS와 동일한 버전으로 빌드된 스파크 버전을 사용해야 한다.  

3. `tar -xf spark-<spark-version>-bin-hadoop<hadoop-version>.tgz` 명령을 사용하여 압축해제.  
4. 스파크 버전과 맞는 스칼라(혹은 파이썬), JDK 설치.

<br/>

---

## interactive shell  

`cd spark-<spark-version>-bin-hadoop<hadoop-version>` 명령을 통해 압축해제한 폴더 내부에서 셸을 실행해보자.  
  - python : `bin/pyspark`  
  - scala : `bin/scala-shell`  

을 통해 각각 언어에 맞는 셸을 실행할 수 있다.

<br/>

---

## standalone application  

### in Scala  

java나 scala에서는 Maven dependency field에 spark-core artifact를 써준다.  

```
groupId = org.apache.spark
artifactId = spark-core_2.11
version = 2.0.0
```  


HDFS cluster에 access하려면 아래 hadoop client artifact도 추가해야 한다.

```
groupId = org.apache.hadoop
artifactId = hadoop-client
version = <your-hdfs-version>
```  


### in python  

python의 실행은 `bin/spark-submit` script를 이용해야 한다.  

```bash
bin/spark-submit my_script.py
```  

<br/>

---

## initializing SparkContext  

### in Scala  

```scala
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._

val conf = new SparkConf().setMaster(master).setAppName(appName)
val sc = SparkContext(conf)
```  

single local machine으로 실행하는 경우, master에 "local"이라고 입력하면 된다.  
이외에는 Spark, Mesos or YARN cluster URL을 입력하면 된다.


### in python

```py
from pyspark import SparkConf, SparkContext

conf = SparkConf().setMaster(master).setAppName(appName)
sc = SparkContext(conf = conf)
```



#### shutdown  

`System.exit(0)`  
`sys.exit()`  
